#  exam 02

##  ai doom

1.  [ai doomsaying](#ai-doomsaying)
2.  [the orthogonality thesis](#the-orthogonality-thesis)
3.  [the instrumental convergence thesis](#the-instrumental-convergence-thesis)
4.  [the decisive advantage thesis](#the-decisive-advantage-thesis)
5.  [the ai doomsayers arguments](#the-ai-doomsayers-arguments)
6.  [the argument for ai++](#the-argument-for-ai++)

##  hau

1.  [trolley](#trolley)
2.  [large man](#large-man)
3.  [hau](#hau)
4.  [hedonic utility](#hedonic-utility)
5.  [maximize utility](#maximize-utility)
6.  [reasons for believing hau](#reasons-for-believing-hau)
7.  [the argument from unjust pleasures](#the-argument-from-unjust-pleasures)
8.  [the argument from worthless pleasures](#the-argument-from-worthless-pleasures)
9.  [the experience machine argument](#the-experience-machine-argument)

##  double effect

1.  [dde](#dde)
2.  [tactical bomber](#tactical-bomber)
3.  [terror / scare bomber](#terror-scare-bomber)
4.  [the argument from modified intentions](#the-argument-from-modified-intentions)
5.  [the normal circumstances solution](#the-normal-circumstances-solution)
6.  [the argument against the normal circumstances solution](#the-argument-against-the-normal-circumstances-solution)


##  ai doom

###  ai doomsaying

this is the view that creating super intelligent machines will lead to the destruction of humanity.

###  the orthogonality thesis

pretty much any amount of intelligence can be combined with pretty much any goal.  there is nothing about intelligence that would make it so that as something gets smarter its goals align with human goals such as the survival of humanity and other human goals such as benevolence, spirituality, intellectual curiosity, etc.

###  the instrumental convergence thesis

whatever goal something has, there are other goals we can predict that it will have in order to achieve its main goal.  for example survival, not losing sight of its main goal, improving its ability to realize its main goal, prefect the technology it has to achieve its main goal, acquire resources to achieve its main goal.

###  the decisive advantage thesis

a super intelligent ai would be able to take over the world.  after all, humans, who are only slightly smarter than other animals, are dominant.  this makes it plausible that super intelligent ai, which is much smarter than humans, could become dominant if that were one of its goals.

###  the ai doomsayers argument

**1.  bostrom's theses are true**<br>
**2.  if bostrom's theses are true, then ai super intelligence will destroy humanity**<br>
**3.  so, ai super intelligence will destroy humanity.**<br>

**motivation for if bostrom's theses are true, then ai super intelligence will destroy humanity**

given the orthogonality thesis, super intelligent ai is not guaranteed, merely in virtue of its intelligence, to care about the survival of humanity.  we might try to design such ai so that it does care about what we care about.  but, ai doomsayer's claim, it is not obvious how we could do that.  like a genie that misses the meaning of a wish because it interprets such wishes excessively literally, rather than as they were intended, it is unclear how we can design ai in such a way that it only does what we want.

given the instrumental convergence thesis, it is very likely that the destruction of humanity would help super intelligent ai achieve its main goal.  for the ai will want to continue improving its ability to accomplish its main goal.  to do so it will need more and more resources.  the conveniently located atoms that make up humans, together with the atoms that make up the resources humans need to survive, could be converted into whatever other form of matter is needed to accomplish the ai's main goal.  finally, given the decisive advantage thesis, its is very likely that we would be unable to stop the ai from converting the matter we need to survive into the matter that is useful to achieve its goal.  so once super intelligent ai is created, humanity is probably doomed.

###  the argument for ai++

1.  there will be ai (before long, absent defeaters)
2.  if there is ai, there will be ai+ (soon after, absent defeaters)
3.  if there is ai+, there will be ai++ (soon after, absent defeaters)
4.  so, there will be ai++ (before too long, absent defeaters)

| type | description |
|:--------|:------------|
| ai      | human level artificial intelligence |
| ai+     | super intelligent artificial intelligence greater than human level (more intelligent than the most intelligent human) |
| ai++    | super intelligence, intelligence greater than human level (at least as far beyond the most intelligent humans as the most intelligent human is beyond a mouse) |

before long, soon after -  doesn't matter decades centuries, etc.

defeaters -  anything that prevents intelligent systems (human or artificial) from manifesting their capacities to create intelligent systems.

**motivation for 1 there will be ai before long, absent defeaters**

emulate the human brain

1.  evolution produced human level intelligence
2.  if evolution produced human level intelligence, then we can produce ai before long.
3.  so absent defeaters, there will be ai before long.

**motivation for 2 if there is ai, then there will be ai+ soon after, absent defeaters**

whenever we come up with a computational product, that product is soon after obsolete due to technological advantages.  we should expect the same to apply to ai.

brain emulation -  emulate a bunch of brains.  but do it on faster hardware and in large clusters.  that will speed up the path to ai+.

other methods will be extendable.  if we do it by direct programming, then, like every other program that has yet been written, the program will be improvable.  that will lead to ai+.

##  hau

hedonic act utilitarianism -  an action is right just in case that action maximizes hedonic utility.

###  hedonic utility

hedonic utility is the virtue that actions have consequences, some of these consequences might include pleasure and some might include pain.  the hedonic utility of an action is the result of subtracting the total amount of pain that is a consequence of the action from the total amount of pleasure that is a consequence of that action.

###  maximize utility

an action maximizes hedonic utility just in case no alternative to that action has a higher hedonic utility.  

###  the argument from unjust pleasures

you are an emperor.  you own a coliseum.  there is a crowd of 10,000 people.  there is one gladiator.  if you release the lions, they will kill and eat the gladiator.  he will suffer 1,000 units of pain.  each member of the crowd will enjoy 1 unit of pleasure.

1.  if hau is true, then you are obligated to release the lions
2.  but you are not obligated to release the lions
3.  so hau is not true

###  the argument from worthless pleasures

1.  if hau is true, you are obligated to let porky into the pig pen
2.  but you are not obligated to let porky into the pig pen
3.  so hau is not true

###  the experience machine argument

1.  if hau is true, then you are obligated to put bob in the experience machine
2.  but you are not obligated to put bob in the experience machine
3.  so hau is not true

##  double effect

###  dde

the doctrine of double effect 

a person may permissibly perform an action that is foreseen to cause an evil if 

1.  the action is not wrong in itself
2.  the good effect is intended
3.  the evil effect is not intended (either as an end or as a means), and
4.  there is proportionally grave reason for permitting the evil

trolley -  1 - 4 are satisfied

a runaway trolley is speeding down a track.  if you do nothing, the trolley will hit five people.  if you pull a lever, the trolley will be diverted onto another track and hit just one person.

large man -  3 is not satisfied

a runaway trolley is speeding down a track.  if you do nothing, the trolley will pass under a tunnel and hit five people.  if you push a large man standing over the entrance of the tunnel, the trolley will hit the large man and stop before it reaches the five.

terror bomber -  3 is not satisfied

aims to bring about civilian deaths as a means to weaken the resolve of the enemy and end the war.

tactical bomber -  1 - 4 are satisfied

aims to destroy military targets in order to end the war.  foresees that the destruction of such targets result in civilian casualties.

###  the argument from modified intentions

1.  if the double effect doctrine is true, then the modified terror bomber and large man pusher didn't do anything wrong
2.  but they did do something wrong
3.  so the double effect doctrine is not true

modified terror bomber -  just like terror bomber with the exception that she doesn't intend to bring about the deaths of the civilians to be the means to ending the war.  she only intends to separate out the bodies of civilians in such a way that they look dead long enough to bring about the belief that they are dead and to speed the end of the war.

modified large man -  just like the large man with the exception that you don't intend to bring about the death of the large man.  you only intend to impact the body of the large man in such a way that the trolley stops before it reaches the five.

###  the normal circumstances solution

there is no intended effect such that under normal circumstances, when that effect happens, something really bad happens.

1.  the action is not wrong in itself
2.  the good effect is intended
3.  there is no intended effect such that under normal circumstances, when that effect happens, something really bad happens
4.  there is proportionally grave reason for permitting the evil

###  the argument against the normal circumstances solution

1.  if the normal circumstances solution is true, then the bomber and the pusher in munitions factory and padded large man did something wrong
2.  but they didn't do anything wrong
3.  so the normal circumstances solution is not true

munitions factor -  normally, munitions factories are located in urban areas near civilians.  if the tactical bomber aims to blow up a munitions factor to end the war, she is aiming to do something that normally results in civilian deaths.

padded large man -  just like large man with the exception that the large man is padded in such a way that the trolley will not harm him.  normally pushing someone into a trolley results in their death or serious injury.